algorithm:
  PPO:
    params:
      learning_rate: 0.0003
      n_steps: 4096
      batch_size: 256
      n_epochs: 10
      gamma: 0.99
      gae_lambda: 0.95
      clip_range: 0.2
      ent_coef: 0.2
      vf_coef: 0.5
      max_grad_norm: 0.5
      normalize_advantage: True
      net_arch: [128, 128, 64]
  
  DDPG:
    params:
      learning_rate: 0.001
      buffer_size: 5000000
      learning_starts: 10000
      batch_size: 256
      tau: 0.01
      gamma: 0.99
      # remember to initialise action noise 
      train_freq: 2
      gradient_steps: 1
      noise_sigma: 0.1
      net_arch: [256, 256, 128]
